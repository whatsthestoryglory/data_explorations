---
title: "Boston Housing Analysis and Predictive Modeling"
author: "Cameron"
date: "3/12/2021"
output: github_document
monofont: "Fira Code"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

LoadLibraries = function() {
  library(ISLR)
  library(MASS)
  library(tidyverse)
  library(caret)
  library(ggpubr)
  library(ggplot2)
  library(viridis)
}

LoadLibraries()
df <- as.data.frame(Boston)

```

## Initial Review

The first thing to do before any sort of modelling is to analyze the dataset and see what you can learn. Some of the things I'm looking for are things like what type of data is in each column (number, string, factors vs continuous, etc.) and whether there are any missing or obviously erroneous values in the data. First, here is the dictionary for our data

        CRIM - per capita crime rate by town
        ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
        INDUS - proportion of non-retail business acres per town.
        CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
        NOX - nitric oxides concentration (parts per 10 million)
        RM - average number of rooms per dwelling
        AGE - proportion of owner-occupied units built prior to 1940
        DIS - weighted distances to five Boston employment centres
        RAD - index of accessibility to radial highways
        TAX - full-value property-tax rate per $10,000
        PTRATIO - pupil-teacher ratio by town
        B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
        LSTAT - % lower status of the population
        MEDV - Median value of owner-occupied homes in $1000's

Next we look at some basic information about how its captured

```{r str}
str(df)
```

This shows us that all of our variables are numeric, with chas and rad being integers. As you can see from the dictionary above, chas is a dummy variable representing whether or not the tract bounds the river, so it's best described as binary in this instance. I'm going to change it to a factor so it doesn't mess with any of our calculations later.

```{r}
#df$chas <- as.factor(df$chas)
```

Rad is listed as an index of accessibility, so I take this as being equivalent to a continuous variable like the others.

```{r summarize}
summary(df)
```

This output gives me a few areas I'd like to investigate further. The zn and chas variables seem to have a lot of 0s so that's something I should follow up on. crim has a median and mean that are very far apart which is also interesting.

```{r histogram, message = FALSE}
library(reshape2)
library(ggplot2)
d <- melt(df)
histo_dist_plot <- ggplot(d,aes(x = value)) + 
    facet_wrap(~variable,scales = "free_x") + 
    geom_histogram() + 
    geom_density(aes(y=..count..)) + ylim(0,75)

histo_dist_plot
```

This output tells us some interesting things. For example, the per capita crime rate by town (`crim`) along with `chas` and `zn` have an overwhelming count of 0s in it. You'll have to trust me on this one as the bars don't show up due to the Y axis I defined to be able to see the shapes of the other histograms. The average number of rooms per dwelling (`rm`) appears to be normally distributed, as does the median home value (`medv`) which is the intended response variable with this dataset. `dis` and `lstat` also appear to have somewhat normal distributions but both have long right tails.

```{r initial_box_plot}
ggplot(d, aes(x=value)) +
  facet_wrap(~variable,scales = "free_x") + 
    geom_boxplot()
```

In some ways I find these boxplots to be more informative than the histogram and density plots shown previously. Here we can see even more strongly the effect of `crim` and `zn` having a large count of 0s as shown by the small box and large quantity of outliers. `black` is interesting as well with the small box to the right and large quantity of outliers to the left.

## Linear Model - the wrong way

```{r initial_linear_model}
lm1 <- lm(medv~., data=df)
summary(lm1)
plot(lm1)
```

From this output we can see that a linear model on the variables as-is actually did pretty well within -2/+1 SD, but is pretty inappropriate outside of those bounds. Looking at the Q-Q or the residuals vs fitted plot you can see that there's a problem with this model. There's a clear U-shaped trend on the residuals and the Q-Q shows a lot of entries far away from the normal line.

Just for fun, though, let's try using a straight linear model with cross-validation to predict the housing values. This will give us a good baseline for future tests to see if we're improving our model or not.

```{r linear_cross_validation}
# Define training control
set.seed(123)
train.control <- trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3)
# Train the model
linear_model <- train(medv ~., data = df, method = "lm",
               trControl = train.control)
# Summarize the results
print(linear_model)
```

So our first attempt we have a RMSE or 4.806 or 21% of the mean of `medv` and a MAE of 3.38. Let's see if we can do any better.

### Stochastic Gradient Boosting

Next we try running a gradient boosting model on it. We're going to use repeated cross-validation here, so it's going to split the data into 10 pieces, train on 9 and test on the 10th; and it's going to do this 3 times.

```{r gam_cross_validation, results='hide'}
# Define training control
set.seed(123)
train.control <- trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3)
# Train the model
gbm_model <- train(medv ~., data = df, method = "gbm",
               trControl = train.control)
```

```{r}
# Summarize the results
print(gbm_model)
```

Here we can see we've improved, we're now at a RMSE of 3.22 and MAE of 2.22. Now that we have a baseline on what can be done with minimal interactions with the data, can we do any better by going back to the data and performing some transformations?

```{r plot_corr_matrix}
cormat <- round(cor(df),2)
library(reshape2)
melted_df <- melt(cormat)
ggplot(data = melted_df, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() +
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
  scale_fill_viridis()
```

In this heatmap we can see that `tax` and `rad` are very highly correlated to one another. Additionaly, `dis` is highly correlated to `indus` `nox` and `age`. Just for fun, let's try running the Stochastic Gradient Boosting with `tax`, `indus`, and `nox` removed due to these strong correlations.

```{r try_removing_correlations, results='hide'}
# Create reduced df
reduced_df <- df %>% select(!c('tax', 'indus', 'nox'))
# Define training control
set.seed(123)
train.control <- trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3)
# Train the model
reduced_model <- train(medv ~., data = reduced_df, method = "gbm",
               trControl = train.control)
```

```{r}
# Summarize the results
print(reduced_model)
```

Our results got worse! RMSE has increased to `3.46` and MAE to `2.40`. Unlike linear models, gradient boosting models tend to not get as caught up with correlated inputs, so this result is not particularly surprising. It was worth a shot, though.

In another notebook on this data I saw a lot of discussion about the top end of `medv`, there are 16 entries where `medv == 50` which is a lot given the otherwise normal nature of the dataset. Let's see what happens if we just bulk remove them.

```{r try_removing_top_medv, results='hide'}
# Create reduced df
removed_df <- df[df$medv < 50,]
# Define training control
set.seed(123)
train.control <- trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3)
# Train the model
removed_top_model <- train(medv ~., data = removed_df, method = "gbm",
               trControl = train.control)


```

```{r}
# Summarize the results
print(removed_top_model)
```

We've improved our best score from RMSE = `r min(reduced_model$results$RMSE)` and MAE = `r min(reduced_model$results$MAE)` to RMSE = `r min(removed_top_model$results$RMSE)` and MAE = `r min(removed_top_model$results$MAE)`. Not bad. What if we now perform a log transformation to remove some of the skew in the data

```{r try_log_on_reduced_df, results='hide'}
# Create reduced df
log_df <- log1p(removed_df%>% select(!medv))
log_df$medv <- removed_df$medv
# Define training control
set.seed(123)
train.control <- trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3)
# Train the model
log_model <- train(medv ~., data = log_df, method = "gbm",
               trControl = train.control)
```

```{r}
# Summarize the results
print(log_model)
```

### That didn't help

Oh well, that one didn't seem to work well. I was pretty ham-fisted with the application of a log to all the predictors. Let's go back and see if being a little more careful does anything to improve matters.

```{r echo=FALSE}
histo_dist_plot 
```

Let's compare this to the log applied dataset.

```{r}
log_d <- melt(log_df)
log_histo_dist_plot <- ggplot(log_d,aes(x = value)) + 
    facet_wrap(~variable,scales = "free_x") + 
    geom_histogram() + 
    geom_density(aes(y=..count..)) + ylim(0,75)

log_histo_dist_plot

```

```{r compare_models}
cat(paste(
  paste("Log Model Performance:", min(log_model$results$RMSE) / mean(log_df$medv)),
  paste("Removed $50k Model Performance:", min(removed_top_model$results$RMSE) / mean(removed_df$medv)),
  paste("Reduced Predictors Performance:", min(reduced_model$results$RMSE) / mean(reduced_df$medv)),
  sep = "\n"
  )
)
```

What I've done here to control for the log transformation is to calculate RMSE / $\\bar{y}$ or Root Mean Square Error divided by the mean of the samples. The results suggest we've made a dramatic improvement in our model by including the log transformation step.

```{r}
cat(paste(
  paste("Log Model RMSE: ", min(log_model$results$RMSE), "  MAE: ", min(log_model$results$MAE)),
  paste("Removed $50k Model RMSE: ", min(removed_top_model$results$RMSE), "  MAE: ", min(removed_top_model$results$MAE)),
  paste("Reduced Predicors Model RMSE: ", min(reduced_model$results$RMSE), "  MAE: ", min(reduced_model$results$MAE)),
  sep = '\n'
))

```

## Results

We have a best RMSE of around `r min(log_model$results$RMSE)` and MAE of `r min(log_model$results$MAE)`. This puts me around 10th place in the latest [Kaggle](https://www.kaggle.com) competition I've seen using this data. That'll be all for now, next I'm going to tackle something with more participants.
